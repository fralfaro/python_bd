{"config":{"lang":["es"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"\ud83d\udc68\u200d\ud83d\udcbb Bienvenido a Python Big Data Engineering","text":""},{"location":"#contenidos-del-curso","title":"\ud83d\udcda Contenidos del Curso","text":"<p>PySpark</p><p>Aprende a procesar grandes vol\u00famenes de datos con Apache Spark usando Python. Domina transformaciones, acciones, Spark SQL y streaming distribuido.</p> <p>Polars</p><p>Descubre una alternativa moderna y ultrarr\u00e1pida a Pandas. Aprende a realizar an\u00e1lisis eficientes en memoria y manipular datos con facilidad.</p>"},{"location":"unit1/","title":"\ud83d\udd25 Fundamentos de PySpark","text":"<p>La Unidad 1 te introduce al mundo del procesamiento distribuido con Apache Spark usando Python. Aqu\u00ed aprender\u00e1s a manejar grandes vol\u00famenes de datos de manera eficiente, utilizando transformaciones, acciones y consultas SQL sobre estructuras distribuidas. Esta unidad es clave para trabajar con Big Data de forma escalable y profesional.</p>"},{"location":"unit1/#objetivos-de-la-unidad","title":"\ud83c\udfaf Objetivos de la unidad","text":"<ul> <li>Comprender el modelo de ejecuci\u00f3n distribuido de Spark.</li> <li>Crear y transformar estructuras de datos distribuidas (RDD y DataFrame).</li> <li>Ejecutar operaciones con <code>transformations</code> y <code>actions</code>.</li> <li>Consultar datos usando Spark SQL.</li> <li>Comprender el flujo de trabajo de un job de Spark.</li> <li>Integrar Spark en entornos locales o en la nube.</li> </ul>"},{"location":"unit1/#contenidos","title":"\ud83d\udcda Contenidos","text":"Tema Descripci\u00f3n breve \u2699\ufe0f Introducci\u00f3n a Spark y PySpark Qu\u00e9 es Spark, c\u00f3mo funciona y c\u00f3mo lo usamos con Python. \ud83d\udce6 RDD y DataFrames Tipos de estructuras distribuidas, diferencias y casos de uso. \ud83d\udd01 Transformaciones y Acciones C\u00f3mo manipular y procesar los datos de forma eficiente. \ud83e\udde0 Lazy Evaluation Ejecuci\u00f3n diferida y optimizaci\u00f3n autom\u00e1tica del plan de ejecuci\u00f3n. \ud83d\udd0d Spark SQL Consultas tipo SQL sobre grandes conjuntos de datos. \ud83e\uddea Spark Streaming (b\u00e1sico) Introducci\u00f3n al procesamiento de datos en tiempo real."},{"location":"unit1/#recomendacion","title":"\ud83e\udde0 Recomendaci\u00f3n","text":"<p>Te sugerimos comenzar por la introducci\u00f3n conceptual y luego avanzar paso a paso por las transformaciones, SQL y finalmente streaming. Aseg\u00farate de ejecutar los ejemplos t\u00fa mismo para entender c\u00f3mo Spark distribuye las operaciones.</p>"},{"location":"unit1/#prerrequisitos","title":"\ud83d\udcda Prerrequisitos","text":"<ul> <li>Conocimientos intermedios de Python</li> <li>Familiaridad con estructuras tipo <code>DataFrame</code></li> <li>Entorno configurado para ejecutar PySpark</li> </ul>"},{"location":"unit1/#que-lograras-al-finalizar-esta-unidad","title":"\u2705 \u00bfQu\u00e9 lograr\u00e1s al finalizar esta unidad?","text":"<ul> <li>Procesar grandes vol\u00famenes de datos con PySpark.</li> <li>Utilizar transformaciones y acciones de forma eficiente.</li> <li>Consultar datos con Spark SQL.</li> <li>Comprender c\u00f3mo funciona un motor de ejecuci\u00f3n distribuido.</li> </ul>"},{"location":"unit1/arquitectura/","title":"Arquitectura de Spark","text":""},{"location":"unit1/arquitectura/#introduccion","title":"Introducci\u00f3n","text":"<p>Comprender c\u00f3mo funciona Apache Spark por dentro es clave para aprovechar al m\u00e1ximo su capacidad de procesamiento distribuido. En este cap\u00edtulo, analizaremos la arquitectura de Spark, sus componentes principales y c\u00f3mo interact\u00faan durante la ejecuci\u00f3n de un trabajo.</p>"},{"location":"unit1/arquitectura/#por-que-es-importante-entender-la-arquitectura","title":"\u00bfPor qu\u00e9 es importante entender la arquitectura?","text":"<p>Spark est\u00e1 dise\u00f1ado para escalar horizontalmente. Esto significa que puede distribuir tareas en m\u00faltiples m\u00e1quinas o n\u00facleos. Saber c\u00f3mo est\u00e1 organizado el motor te permitir\u00e1:</p> <ul> <li>Optimizar la ejecuci\u00f3n de tus scripts.</li> <li>Comprender mejor errores y cuellos de botella.</li> <li>Tomar decisiones informadas sobre configuraci\u00f3n y recursos.</li> </ul>"},{"location":"unit1/arquitectura/#componentes-principales-de-spark","title":"Componentes principales de Spark","text":"<p>La arquitectura de Spark se basa en un modelo maestro-trabajador (master-worker). A continuaci\u00f3n se describen sus elementos clave:</p>"},{"location":"unit1/arquitectura/#1-driver","title":"1. Driver","text":"<ul> <li>Es el programa principal que ejecutas.</li> <li>Contiene tu c\u00f3digo PySpark y gestiona la ejecuci\u00f3n del trabajo.</li> <li>Coordina la comunicaci\u00f3n con el resto del cl\u00faster.</li> </ul>"},{"location":"unit1/arquitectura/#2-cluster-manager","title":"2. Cluster Manager","text":"<ul> <li>Es el orquestador del cl\u00faster.</li> <li>Se encarga de asignar recursos a la aplicaci\u00f3n Spark.</li> <li>Puede ser <code>Standalone</code>, <code>YARN</code>, <code>Mesos</code>, <code>Kubernetes</code>, etc.</li> </ul>"},{"location":"unit1/arquitectura/#3-executors","title":"3. Executors","text":"<ul> <li>Son procesos que corren en los nodos del cl\u00faster.</li> <li>Ejecutan las tareas asignadas por el Driver.</li> <li>Cada executor tiene su propia memoria y cach\u00e9.</li> </ul>"},{"location":"unit1/arquitectura/#4-tasks","title":"4. Tasks","text":"<ul> <li>Son las unidades m\u00e1s peque\u00f1as de trabajo que ejecutan operaciones sobre particiones de datos.</li> <li>Son asignadas por el Driver a los Executors.</li> </ul>"},{"location":"unit1/arquitectura/#diagrama-logico","title":"Diagrama l\u00f3gico","text":"<pre><code>   +------------------+\n   |      Driver      |\n   +--------+---------+\n            |\n            v\n   +------------------+\n   | Cluster Manager  |\n   +--------+---------+\n            |\n    +-------+-------+\n    |               |\n    v               v\n+--------+     +--------+\n|Executor|     |Executor|\n| Task 1 |     | Task 2 |\n| Task 3 |     | Task 4 |\n+--------+     +--------+\n</code></pre>"},{"location":"unit1/arquitectura/#que-es-un-dag","title":"\u00bfQu\u00e9 es un DAG?","text":"<p>Spark representa cada trabajo como un DAG (Directed Acyclic Graph) de transformaciones. Esto permite:</p> <ul> <li>Planificar la ejecuci\u00f3n antes de ejecutarla.</li> <li>Optimizar el orden de las operaciones.</li> <li>Evitar operaciones innecesarias (como c\u00e1lculos duplicados).</li> </ul> <p>Un DAG no es m\u00e1s que una estructura de nodos y flechas donde cada nodo representa una operaci\u00f3n, y las flechas muestran la dependencia entre ellas.</p>"},{"location":"unit1/arquitectura/#modo-de-ejecucion-etapas-y-tareas","title":"Modo de ejecuci\u00f3n: etapas y tareas","text":"<p>Al ejecutar una acci\u00f3n (como <code>.count()</code> o <code>.collect()</code>), Spark:</p> <ol> <li>Construye un DAG a partir de las transformaciones encadenadas.</li> <li>Divide el DAG en etapas (stages), donde cada etapa puede ser ejecutada en paralelo.</li> <li>Dentro de cada etapa, genera m\u00faltiples tareas (tasks), una por cada partici\u00f3n.</li> <li>Env\u00eda estas tareas a los executors, los cuales las procesan y devuelven los resultados al driver.</li> </ol>"},{"location":"unit1/arquitectura/#ejemplo-de-ejecucion","title":"Ejemplo de ejecuci\u00f3n","text":"<pre><code>from pyspark.sql import SparkSession\n\nspark = SparkSession.builder.appName(\"EjemploDAG\").getOrCreate()\ndf = spark.read.csv(\"data/ventas.csv\", header=True, inferSchema=True)\n\ndf_filtrado = df.filter(df[\"monto\"] &gt; 1000)\ndf_filtrado.select(\"cliente\", \"monto\").show()\n</code></pre> <p>En este ejemplo:</p> <ul> <li>El DAG se construye con <code>.filter()</code> y <code>.select()</code>.</li> <li>La acci\u00f3n <code>.show()</code> dispara la ejecuci\u00f3n.</li> <li>Spark calcula el plan \u00f3ptimo, lo divide en etapas y ejecuta en paralelo.</li> </ul>"},{"location":"unit1/arquitectura/#referencias-utiles","title":"Referencias \u00fatiles","text":"<ul> <li>Apache Spark Overview</li> <li>Understanding the DAG in Spark</li> <li>Deploying Spark on Kubernetes</li> </ul>"},{"location":"unit1/arquitectura/#conclusion","title":"Conclusi\u00f3n","text":"<p>La arquitectura de Spark est\u00e1 pensada para escalar y optimizar autom\u00e1ticamente las operaciones sobre grandes vol\u00famenes de datos. Comprender c\u00f3mo se distribuye el trabajo entre el driver, el cluster manager y los executors te ayudar\u00e1 a escribir c\u00f3digo m\u00e1s eficiente y depurar errores de forma m\u00e1s efectiva.</p> <p>En el siguiente cap\u00edtulo aprender\u00e1s c\u00f3mo instalar PySpark en tu entorno local o en la nube, y c\u00f3mo preparar tu entorno de trabajo.</p>"},{"location":"unit1/dataframes/","title":"DataFrames en PySpark","text":""},{"location":"unit1/dataframes/#introduccion","title":"Introducci\u00f3n","text":"<p>Los <code>DataFrames</code> son una de las estructuras fundamentales para trabajar con datos en PySpark. Inspirados en los <code>DataFrames</code> de Pandas y R, ofrecen una forma eficiente y expresiva de manipular grandes vol\u00famenes de datos distribuidos.</p> <p>En este cap\u00edtulo, aprender\u00e1s c\u00f3mo crear, explorar y transformar DataFrames usando PySpark.</p>"},{"location":"unit1/dataframes/#que-es-un-dataframe-en-pyspark","title":"\u00bfQu\u00e9 es un DataFrame en PySpark?","text":"<p>Un <code>DataFrame</code> en PySpark es una colecci\u00f3n distribuida de datos organizados en columnas, con un esquema expl\u00edcito (nombres y tipos de datos). Internamente, est\u00e1 optimizado para ejecuci\u00f3n distribuida en cl\u00fasteres, soporta ejecuci\u00f3n diferida (lazy evaluation) y se integra con SQL.</p>"},{"location":"unit1/dataframes/#crear-un-dataframe-a-partir-de-un-archivo-csv","title":"Crear un DataFrame a partir de un archivo CSV","text":"<p>La forma m\u00e1s com\u00fan de iniciar un an\u00e1lisis es cargar un archivo desde disco:</p> <pre><code>df = spark.read.csv(\"data/ventas.csv\", header=True, inferSchema=True)\n</code></pre> <p>Par\u00e1metros comunes:</p> <ul> <li><code>header=True</code>: toma la primera fila como nombres de columnas.</li> <li><code>inferSchema=True</code>: detecta autom\u00e1ticamente los tipos de datos.</li> </ul>"},{"location":"unit1/dataframes/#crear-un-dataframe-manualmente","title":"Crear un DataFrame manualmente","text":"<p>Tambi\u00e9n puedes crear un DataFrame desde una lista de tuplas y una lista de nombres de columnas:</p> <pre><code>datos = [(\"Ana\", 25), (\"Luis\", 30), (\"Pedro\", 28)]\ncolumnas = [\"nombre\", \"edad\"]\n\ndf = spark.createDataFrame(datos, columnas)\ndf.show()\n</code></pre>"},{"location":"unit1/dataframes/#explorar-el-contenido-del-dataframe","title":"Explorar el contenido del DataFrame","text":"<p>Algunas funciones \u00fatiles para inspeccionar tus datos:</p> <pre><code>df.show(5)         # Muestra las primeras 5 filas\ndf.printSchema()   # Muestra el esquema (nombres y tipos de columnas)\ndf.columns         # Lista de nombres de columnas\ndf.describe().show()  # Estad\u00edsticas descriptivas\n</code></pre>"},{"location":"unit1/dataframes/#seleccionar-columnas-y-filtrar-filas","title":"Seleccionar columnas y filtrar filas","text":"<pre><code># Seleccionar una o varias columnas\ndf.select(\"nombre\").show()\ndf.select(\"nombre\", \"edad\").show()\n\n# Filtrar filas\ndf.filter(df[\"edad\"] &gt; 27).show()\ndf.where(df[\"nombre\"] == \"Ana\").show()\n</code></pre>"},{"location":"unit1/dataframes/#agregaciones-y-funciones-de-grupo","title":"Agregaciones y funciones de grupo","text":"<pre><code>df.groupBy(\"edad\").count().show()\ndf.agg({\"edad\": \"avg\"}).show()\n</code></pre>"},{"location":"unit1/dataframes/#ordenar-y-renombrar-columnas","title":"Ordenar y renombrar columnas","text":"<pre><code>df.orderBy(\"edad\", ascending=False).show()\ndf = df.withColumnRenamed(\"edad\", \"edad_a\u00f1os\")\n</code></pre>"},{"location":"unit1/dataframes/#anadir-columnas-derivadas","title":"A\u00f1adir columnas derivadas","text":"<p>Puedes crear nuevas columnas a partir de otras usando expresiones:</p> <pre><code>from pyspark.sql.functions import col\n\ndf = df.withColumn(\"edad_doble\", col(\"edad\") * 2)\ndf.show()\n</code></pre>"},{"location":"unit1/dataframes/#eliminar-columnas","title":"Eliminar columnas","text":"<pre><code>df = df.drop(\"edad_doble\")\n</code></pre>"},{"location":"unit1/dataframes/#uniones-entre-dataframes","title":"Uniones entre DataFrames","text":"<pre><code>df1 = spark.createDataFrame([(\"Ana\", 1), (\"Luis\", 2)], [\"nombre\", \"id\"])\ndf2 = spark.createDataFrame([(1, \"Chile\"), (2, \"Per\u00fa\")], [\"id\", \"pais\"])\n\ndf_join = df1.join(df2, on=\"id\", how=\"inner\")\ndf_join.show()\n</code></pre> <p>Tipos de join disponibles: <code>inner</code>, <code>left</code>, <code>right</code>, <code>outer</code>.</p>"},{"location":"unit1/dataframes/#conversion-a-pandas-solo-si-es-pequeno","title":"Conversi\u00f3n a Pandas (solo si es peque\u00f1o)","text":"<pre><code>df_pandas = df.toPandas()\n</code></pre> <p>\u00dasalo solo si el conjunto de datos es peque\u00f1o y cabe en memoria.</p>"},{"location":"unit1/dataframes/#referencias-utiles","title":"Referencias \u00fatiles","text":"<ul> <li>PySpark DataFrame API</li> <li>Spark SQL Guide</li> <li>Transformations vs Actions</li> </ul>"},{"location":"unit1/dataframes/#conclusion","title":"Conclusi\u00f3n","text":"<p>Los DataFrames en PySpark son una herramienta poderosa y expresiva para procesar datos distribuidos. Dominarlos es clave para trabajar de forma eficiente con grandes vol\u00famenes de informaci\u00f3n. A diferencia de Pandas, los DataFrames de PySpark est\u00e1n dise\u00f1ados para escalar horizontalmente, permitiendo an\u00e1lisis que superan las capacidades de una sola m\u00e1quina.</p> <p>En el siguiente cap\u00edtulo aprender\u00e1s a aplicar transformaciones y acciones, que son los bloques fundamentales del procesamiento en Spark.</p>"},{"location":"unit1/introduction/","title":"Introducci\u00f3n a PySpark","text":""},{"location":"unit1/introduction/#introduccion","title":"Introducci\u00f3n","text":"<p>Apache Spark es uno de los motores de procesamiento m\u00e1s potentes y utilizados en el ecosistema Big Data. Su capacidad para procesar grandes vol\u00famenes de datos de manera distribuida, r\u00e1pida y en memoria lo convierte en una herramienta clave en la ingenier\u00eda de datos moderna.</p> <p>En este cap\u00edtulo, exploraremos qu\u00e9 es PySpark, por qu\u00e9 es relevante en proyectos de datos a gran escala y c\u00f3mo se posiciona frente a otras herramientas tradicionales como Pandas.</p>"},{"location":"unit1/introduction/#que-es-apache-spark","title":"\u00bfQu\u00e9 es Apache Spark?","text":"<p>Apache Spark es un motor de an\u00e1lisis de datos distribuido de c\u00f3digo abierto, dise\u00f1ado para procesar grandes vol\u00famenes de datos de manera paralela y eficiente.</p> <p>Fue creado en la Universidad de California, Berkeley, como una evoluci\u00f3n del modelo MapReduce de Hadoop. A diferencia de este \u00faltimo, Spark permite procesamiento en memoria (in-memory), lo que reduce significativamente los tiempos de ejecuci\u00f3n.</p> <p>Spark permite trabajar con distintos tipos de procesamiento:</p> <ul> <li>Batch (por lotes)</li> <li>Streaming (en tiempo real)</li> <li>Machine Learning</li> <li>SQL</li> <li>Gr\u00e1ficos (GraphX)</li> </ul>"},{"location":"unit1/introduction/#que-es-pyspark","title":"\u00bfQu\u00e9 es PySpark?","text":"<p>PySpark es la interfaz de Python para Apache Spark. Gracias a PySpark, los desarrolladores Python pueden escribir c\u00f3digo para trabajar con Spark sin necesidad de utilizar Scala o Java.</p> <p>Entre sus capacidades m\u00e1s destacadas est\u00e1n:</p> <ul> <li>Trabajar con DataFrames distribuidos (similar a Pandas, pero escalable).</li> <li>Ejecutar operaciones SQL con <code>Spark SQL</code>.</li> <li>Procesar datos en tiempo real con <code>Structured Streaming</code>.</li> <li>Realizar tareas de machine learning con <code>MLlib</code>.</li> </ul> <p>\ud83d\udd27 PySpark traduce las instrucciones escritas en Python al n\u00facleo de ejecuci\u00f3n de Spark, lo que permite trabajar a gran escala sin perder la flexibilidad de Python.</p>"},{"location":"unit1/introduction/#por-que-usar-pyspark-en-lugar-de-pandas","title":"\u00bfPor qu\u00e9 usar PySpark en lugar de Pandas?","text":"<p>Aunque Pandas es excelente para an\u00e1lisis de datos en memoria, presenta limitaciones cuando:</p> <ul> <li>Los datos superan los 8-16GB (el tama\u00f1o de la RAM).</li> <li>Se requiere procesamiento paralelo o distribuido.</li> <li>Se necesita conectar m\u00faltiples fuentes de datos grandes.</li> </ul> Caracter\u00edstica Pandas PySpark Tama\u00f1o de datos En memoria (limitado) Escalable a cl\u00faster Paralelismo No S\u00ed, distribuido Tipado Din\u00e1mico Est\u00e1tico (inferido) Tiempo de carga R\u00e1pido en peque\u00f1os Optimizado para grandes Lenguaje base Python puro Python sobre Spark (Scala)"},{"location":"unit1/introduction/#casos-de-uso-tipicos-para-pyspark","title":"Casos de uso t\u00edpicos para PySpark","text":"<ul> <li>Procesamiento de logs de servidores o aplicaciones.</li> <li>Limpieza y transformaci\u00f3n de datasets de terabytes.</li> <li>Preparaci\u00f3n de datos para modelos de machine learning distribuidos.</li> <li>An\u00e1lisis de comportamiento de usuarios en tiempo real.</li> <li>Consultas complejas sobre archivos Parquet, ORC, CSV distribuidos.</li> </ul>"},{"location":"unit1/introduction/#primer-vistazo-al-codigo-pyspark","title":"Primer vistazo al c\u00f3digo PySpark","text":"<p>Este es un ejemplo b\u00e1sico para iniciar una sesi\u00f3n de Spark y leer un archivo:</p> <pre><code>from pyspark.sql import SparkSession\n\n# Crear una SparkSession\nspark = SparkSession.builder.appName(\"MiPrimerPySpark\").getOrCreate()\n\n# Leer un CSV (puede ser local o desde Google Drive)\ndf = spark.read.csv(\"data/ventas.csv\", header=True, inferSchema=True)\n\n# Mostrar las primeras filas\ndf.show()\n</code></pre> <p>\ud83e\uddea Puedes probar este ejemplo f\u00e1cilmente en Google Colab con <code>!pip install pyspark</code>.</p>"},{"location":"unit1/introduction/#ventajas-principales-de-pyspark","title":"Ventajas principales de PySpark","text":"<ul> <li>Escalabilidad horizontal: puedes escalar tu trabajo desde tu laptop hasta cientos de nodos en un cl\u00faster.</li> <li>Lenguaje conocido: no necesitas aprender Java o Scala.</li> <li>Ecosistema rico: acceso a Spark SQL, MLlib, GraphFrames, entre otros.</li> <li>Interoperabilidad: puede conectarse con Hadoop, Hive, Kafka, Cassandra, Delta Lake, etc.</li> </ul>"},{"location":"unit1/introduction/#referencias-utiles","title":"Referencias \u00fatiles","text":"<ul> <li>Apache Spark - Sitio oficial</li> <li>Documentaci\u00f3n de PySpark</li> <li>Comparativa PySpark vs Pandas (Databricks)</li> <li>Gu\u00eda de instalaci\u00f3n en Colab (pyspark)</li> </ul>"},{"location":"unit1/introduction/#conclusion","title":"Conclusi\u00f3n","text":"<p>PySpark es una herramienta fundamental en la ingenier\u00eda de datos moderna. Su combinaci\u00f3n de potencia, escalabilidad y compatibilidad con Python lo convierten en una opci\u00f3n natural para procesar y analizar grandes vol\u00famenes de datos.</p> <p>Dominar PySpark abre la puerta a trabajar con proyectos de Big Data reales, donde Pandas ya no es suficiente. En los pr\u00f3ximos cap\u00edtulos, aprender\u00e1s a construir DataFrames, ejecutar transformaciones distribuidas y optimizar tus consultas como un profesional.</p>"},{"location":"unit1/lazy/","title":"Lazy Evaluation","text":""},{"location":"unit1/lazy/#introduccion","title":"Introducci\u00f3n","text":"<p>Uno de los aspectos m\u00e1s poderosos (y menos intuitivos al principio) de PySpark es su modelo de evaluaci\u00f3n diferida, conocido como lazy evaluation. A diferencia de otras librer\u00edas como Pandas, donde cada operaci\u00f3n se ejecuta de inmediato, en PySpark las transformaciones no se ejecutan hasta que se llama expl\u00edcitamente a una acci\u00f3n.</p> <p>En este cap\u00edtulo aprender\u00e1s qu\u00e9 significa esto, c\u00f3mo funciona internamente y c\u00f3mo aprovecharlo para escribir c\u00f3digo m\u00e1s eficiente.</p>"},{"location":"unit1/lazy/#que-es-la-evaluacion-diferida","title":"\u00bfQu\u00e9 es la evaluaci\u00f3n diferida?","text":"<p>Cuando encadenas varias transformaciones en un DataFrame de PySpark, nada se ejecuta en ese momento. En su lugar, Spark construye internamente un DAG (grafo ac\u00edclico dirigido) con el plan de ejecuci\u00f3n.</p> <p>Este DAG es evaluado solo cuando llamas a una acci\u00f3n, como <code>show()</code>, <code>count()</code>, <code>collect()</code>, entre otras.</p> <p>Esto permite a Spark aplicar optimizaciones autom\u00e1ticas, como:</p> <ul> <li>Reordenar operaciones para mejorar el rendimiento.</li> <li>Omitir c\u00e1lculos innecesarios.</li> <li>Combinar etapas para reducir movimientos de datos.</li> </ul>"},{"location":"unit1/lazy/#ejemplo-ejecucion-diferida","title":"Ejemplo: ejecuci\u00f3n diferida","text":"<pre><code>from pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col\n\nspark = SparkSession.builder.appName(\"LazyEvaluationDemo\").getOrCreate()\n\ndf = spark.read.csv(\"data/ventas.csv\", header=True, inferSchema=True)\n\n# Transformaciones (no ejecutan nada a\u00fan)\ndf_filtrado = df.filter(col(\"monto\") &gt; 1000)\ndf_seleccion = df_filtrado.select(\"cliente\", \"monto\")\n\n# Acci\u00f3n que dispara la ejecuci\u00f3n\ndf_seleccion.show()\n</code></pre> <p>En este ejemplo, la lectura del archivo y las transformaciones no se ejecutan hasta que se llama a <code>.show()</code>. En ese momento, Spark analiza el plan, aplica optimizaciones y ejecuta las transformaciones de forma distribuida.</p>"},{"location":"unit1/lazy/#visualizar-el-plan-de-ejecucion","title":"Visualizar el plan de ejecuci\u00f3n","text":"<p>Puedes ver el plan que Spark genera con el m\u00e9todo <code>.explain()</code>:</p> <pre><code>df_seleccion.explain()\n</code></pre> <p>Esto muestra el plan l\u00f3gico y f\u00edsico, incluyendo las etapas, operadores y particiones. Es \u00fatil para depurar y optimizar tu c\u00f3digo.</p>"},{"location":"unit1/lazy/#por-que-es-util","title":"\u00bfPor qu\u00e9 es \u00fatil?","text":"<ul> <li>Eficiencia: Spark evita pasos innecesarios y reduce el uso de memoria.</li> <li>Escalabilidad: El motor distribuye mejor las tareas cuando tiene una visi\u00f3n completa del flujo.</li> <li>Modularidad: Puedes construir pipelines complejos sin preocuparte por cu\u00e1ndo se ejecuta cada paso.</li> </ul>"},{"location":"unit1/lazy/#comparacion-con-pandas","title":"Comparaci\u00f3n con Pandas","text":"Caracter\u00edstica Pandas PySpark Ejecuci\u00f3n Inmediata Diferida (lazy) Optimizaci\u00f3n autom\u00e1tica No S\u00ed Planificaci\u00f3n global No (paso a paso) S\u00ed (plan de ejecuci\u00f3n DAG) Escalabilidad Limitada (RAM local) Distribuida en cl\u00faster"},{"location":"unit1/lazy/#buenas-practicas","title":"Buenas pr\u00e1cticas","text":"<ul> <li>Encadena transformaciones: Es m\u00e1s eficiente definir varias operaciones seguidas antes de ejecutar una acci\u00f3n.</li> <li>Evita m\u00faltiples acciones innecesarias: Cada acci\u00f3n genera una ejecuci\u00f3n completa.</li> <li>Usa <code>.explain()</code> para entender c\u00f3mo Spark ejecutar\u00e1 tu c\u00f3digo.</li> <li>Ten cuidado con <code>.collect()</code> o <code>.toPandas()</code>, ya que traer\u00e1n todos los datos al driver, ejecutando todo el DAG.</li> </ul>"},{"location":"unit1/lazy/#referencias-utiles","title":"Referencias \u00fatiles","text":"<ul> <li>Spark Programming Guide: RDDs and Lazy Evaluation</li> <li>Optimizing Spark with Lazy Evaluation</li> <li>PySpark DataFrame API</li> </ul>"},{"location":"unit1/lazy/#conclusion","title":"Conclusi\u00f3n","text":"<p>La evaluaci\u00f3n diferida es uno de los pilares de Spark. Gracias a ella, Spark puede optimizar tus operaciones y ejecutar solo lo necesario, de forma paralela y distribuida. Entender este modelo es esencial para escribir c\u00f3digo eficiente, escalar tus an\u00e1lisis y evitar errores de rendimiento.</p>"},{"location":"unit1/rdd/","title":"RDD en PySpark","text":""},{"location":"unit1/rdd/#introduccion","title":"Introducci\u00f3n","text":"<p>Antes de la introducci\u00f3n de los DataFrames, Spark se basaba en una estructura fundamental llamada RDD (Resilient Distributed Dataset). Aunque su uso ha disminuido en aplicaciones modernas, los RDDs siguen siendo relevantes para operaciones de bajo nivel y control m\u00e1s expl\u00edcito sobre la ejecuci\u00f3n distribuida.</p> <p>En este cap\u00edtulo exploraremos qu\u00e9 son los RDDs, c\u00f3mo se crean, cu\u00e1ndo son \u00fatiles y c\u00f3mo se comparan con las APIs m\u00e1s modernas de PySpark.</p>"},{"location":"unit1/rdd/#que-es-un-rdd","title":"\u00bfQu\u00e9 es un RDD?","text":"<p>Un RDD es una colecci\u00f3n inmutable y distribuida de objetos que puede ser procesada en paralelo. Cada partici\u00f3n del RDD puede residir en un nodo distinto del cl\u00faster.</p> <p>Caracter\u00edsticas clave:</p> <ul> <li>Distribuido: fragmentado autom\u00e1ticamente en m\u00faltiples nodos.</li> <li>Inmutable: cualquier transformaci\u00f3n crea un nuevo RDD.</li> <li>Perecedero: se puede volver a calcular desde el origen si un nodo falla.</li> <li>Tolerante a fallos: gracias al modelo de linaje (DAG).</li> </ul>"},{"location":"unit1/rdd/#cuando-usar-rdd","title":"\u00bfCu\u00e1ndo usar RDD?","text":"<p>Aunque los DataFrames y Datasets son preferidos por simplicidad y rendimiento, los RDDs son \u00fatiles cuando:</p> <ul> <li>Necesitas control fino sobre las particiones o el flujo de datos.</li> <li>Trabajas con datos no estructurados o transformaciones personalizadas.</li> <li>Implementas algoritmos complejos que no encajan bien en SQL o DataFrames.</li> </ul>"},{"location":"unit1/rdd/#crear-un-rdd-en-pyspark","title":"Crear un RDD en PySpark","text":"<pre><code>from pyspark.sql import SparkSession\n\nspark = SparkSession.builder.appName(\"RDDDemo\").getOrCreate()\nsc = spark.sparkContext  # Acceso al contexto de Spark (nivel bajo)\n\n# Crear un RDD desde una lista\nrdd = sc.parallelize([1, 2, 3, 4, 5])\n\n# Ver los elementos\nprint(rdd.collect())\n</code></pre>"},{"location":"unit1/rdd/#transformaciones-y-acciones-en-rdd","title":"Transformaciones y acciones en RDD","text":"<p>Las operaciones en RDD siguen el mismo modelo de transformaci\u00f3n + acci\u00f3n que los DataFrames.</p>"},{"location":"unit1/rdd/#transformaciones-comunes","title":"Transformaciones comunes:","text":"<ul> <li><code>map()</code></li> <li><code>filter()</code></li> <li><code>flatMap()</code></li> <li><code>union()</code></li> <li><code>distinct()</code></li> </ul>"},{"location":"unit1/rdd/#acciones-comunes","title":"Acciones comunes:","text":"<ul> <li><code>collect()</code></li> <li><code>count()</code></li> <li><code>first()</code></li> <li><code>reduce()</code></li> <li><code>take(n)</code></li> </ul> <p>Ejemplo:</p> <pre><code>pares = rdd.filter(lambda x: x % 2 == 0)\ncuadrados = pares.map(lambda x: x * x)\nprint(cuadrados.collect())  # [4, 16]\n</code></pre>"},{"location":"unit1/rdd/#rdd-vs-dataframe","title":"RDD vs DataFrame","text":"Caracter\u00edstica RDD DataFrame Nivel de abstracci\u00f3n Bajo Alto Tipo de datos Objetos Python gen\u00e9ricos Tablas con columnas Optimizaci\u00f3n autom\u00e1tica No S\u00ed (Catalyst Optimizer) Tipado No estructurado Estructurado con schema Rendimiento M\u00e1s bajo M\u00e1s alto Flexibilidad Alta Limitada a operaciones tabulares"},{"location":"unit1/rdd/#casos-donde-un-rdd-aun-es-util","title":"Casos donde un RDD a\u00fan es \u00fatil","text":"<ul> <li>Procesamiento de logs no estructurados.</li> <li>Algoritmos personalizados (como PageRank).</li> <li>Cuando necesitas granularidad sobre c\u00f3mo se manejan las particiones.</li> <li>Operaciones que requieren acceso a nivel de fila completo sin esquema.</li> </ul>"},{"location":"unit1/rdd/#conversion-entre-rdd-y-dataframe","title":"Conversi\u00f3n entre RDD y DataFrame","text":"<p>Puedes convertir un RDD a DataFrame si defines el esquema:</p> <pre><code>from pyspark.sql import Row\n\nrdd = sc.parallelize([Row(nombre=\"Ana\", edad=30), Row(nombre=\"Luis\", edad=25)])\ndf = spark.createDataFrame(rdd)\ndf.show()\n</code></pre> <p>Y tambi\u00e9n puedes ir de DataFrame a RDD:</p> <pre><code>df_rdd = df.rdd\n</code></pre>"},{"location":"unit1/rdd/#referencias-utiles","title":"Referencias \u00fatiles","text":"<ul> <li>RDD Programming Guide (Apache Spark)</li> <li>Transformations and Actions in RDD</li> <li>PySpark RDD API</li> </ul>"},{"location":"unit1/rdd/#conclusion","title":"Conclusi\u00f3n","text":"<p>Los RDDs representan la base original de Spark y siguen siendo \u00fatiles para tareas donde se necesita control detallado sobre los datos o cuando las estructuras tabulares no son suficientes. Sin embargo, para la mayor\u00eda de los casos pr\u00e1cticos, especialmente en proyectos empresariales, se recomienda usar DataFrames por simplicidad y eficiencia.</p> <p>Conocer RDD te ayuda a entender mejor c\u00f3mo Spark opera internamente y te prepara para resolver casos avanzados o no convencionales.</p>"},{"location":"unit1/spark_session/","title":"Crear una SparkSession","text":""},{"location":"unit1/spark_session/#introduccion","title":"Introducci\u00f3n","text":"<p>Antes de comenzar a trabajar con PySpark, necesitas iniciar una sesi\u00f3n de Spark. Esta sesi\u00f3n es el punto de entrada para acceder a todas las funcionalidades del motor, como leer datos, aplicar transformaciones, ejecutar SQL o hacer streaming.</p> <p>En este cap\u00edtulo aprender\u00e1s qu\u00e9 es la <code>SparkSession</code>, c\u00f3mo configurarla correctamente y c\u00f3mo usarla para cargar datos por primera vez.</p>"},{"location":"unit1/spark_session/#que-es-una-sparksession","title":"\u00bfQu\u00e9 es una SparkSession?","text":"<p>La <code>SparkSession</code> es el objeto principal para interactuar con el motor de Spark. A partir de Spark 2.0, reemplaza a objetos como <code>SQLContext</code> y <code>HiveContext</code>, y proporciona una interfaz unificada para trabajar con:</p> <ul> <li>DataFrames</li> <li>SQL</li> <li>UDFs (funciones definidas por el usuario)</li> <li>Streaming</li> <li>Configuraciones del entorno</li> </ul>"},{"location":"unit1/spark_session/#crear-una-sparksession_1","title":"Crear una SparkSession","text":"<p>En la mayor\u00eda de los casos, bastar\u00e1 con el siguiente bloque para iniciar tu sesi\u00f3n:</p> <pre><code>from pyspark.sql import SparkSession\n\nspark = SparkSession.builder \\\n    .appName(\"MiAplicacion\") \\\n    .getOrCreate()\n</code></pre> <p>Esto crear\u00e1 una sesi\u00f3n con la configuraci\u00f3n por defecto. Si ya existe una sesi\u00f3n activa, <code>getOrCreate()</code> la reutilizar\u00e1.</p>"},{"location":"unit1/spark_session/#parametros-comunes-en-builder","title":"Par\u00e1metros comunes en <code>builder</code>","text":"<p>Puedes personalizar tu sesi\u00f3n usando m\u00e9todos encadenados:</p> <pre><code>spark = SparkSession.builder \\\n    .appName(\"MiProyecto\") \\\n    .master(\"local[*]\") \\\n    .config(\"spark.executor.memory\", \"2g\") \\\n    .getOrCreate()\n</code></pre> Par\u00e1metro Descripci\u00f3n <code>.appName()</code> Nombre de la aplicaci\u00f3n (aparece en logs y UI). <code>.master()</code> Modo de ejecuci\u00f3n (<code>local[*]</code>, <code>yarn</code>, etc.). <code>.config()</code> A\u00f1ade configuraciones personalizadas."},{"location":"unit1/spark_session/#verificar-que-spark-esta-activo","title":"Verificar que Spark est\u00e1 activo","text":"<p>Una vez creada la sesi\u00f3n, puedes consultar algunas propiedades \u00fatiles:</p> <pre><code>print(spark.version)              # Versi\u00f3n de Spark\nprint(spark.sparkContext.appName) # Nombre de la aplicaci\u00f3n\nprint(spark.sparkContext.master)  # Modo de ejecuci\u00f3n\n</code></pre>"},{"location":"unit1/spark_session/#cargar-un-archivo-csv","title":"Cargar un archivo CSV","text":"<p>Veamos un ejemplo simple para leer un archivo CSV y mostrar su contenido:</p> <pre><code>df = spark.read.csv(\"data/ventas.csv\", header=True, inferSchema=True)\ndf.show(5)\n</code></pre> <ul> <li><code>header=True</code> indica que la primera fila contiene los nombres de las columnas.</li> <li><code>inferSchema=True</code> le dice a Spark que detecte autom\u00e1ticamente los tipos de datos.</li> </ul>"},{"location":"unit1/spark_session/#apagar-la-sesion","title":"Apagar la sesi\u00f3n","text":"<p>Al final del programa o del notebook, puedes cerrar la sesi\u00f3n con:</p> <pre><code>spark.stop()\n</code></pre> <p>Esto libera los recursos usados por Spark y evita que se mantenga en memoria innecesariamente.</p>"},{"location":"unit1/spark_session/#buenas-practicas","title":"Buenas pr\u00e1cticas","text":"<ul> <li>Usa <code>getOrCreate()</code> para evitar errores si ya existe una sesi\u00f3n activa.</li> <li>Evita crear m\u00faltiples sesiones en el mismo notebook.</li> <li>Configura <code>master(\"local[*]\")</code> si est\u00e1s trabajando en local con m\u00faltiples n\u00facleos.</li> <li>Nombra bien tu aplicaci\u00f3n con <code>.appName()</code> para facilitar la depuraci\u00f3n.</li> </ul>"},{"location":"unit1/spark_session/#referencias-utiles","title":"Referencias \u00fatiles","text":"<ul> <li>API de SparkSession</li> <li>Configuraci\u00f3n de Spark</li> <li>Gu\u00eda de lectura de archivos</li> </ul>"},{"location":"unit1/spark_session/#conclusion","title":"Conclusi\u00f3n","text":"<p>La <code>SparkSession</code> es el primer paso para comenzar a trabajar con PySpark. Saber c\u00f3mo crearla, configurarla y utilizarla correctamente te permitir\u00e1 manipular datos distribuidos y comenzar a explorar los componentes clave de Spark.</p> <p>En el siguiente cap\u00edtulo, aprender\u00e1s c\u00f3mo trabajar con estructuras fundamentales como los <code>DataFrames</code>, una de las herramientas m\u00e1s potentes para manipular datos en PySpark.</p>"},{"location":"unit1/sparksql/","title":"Consultas con Spark SQL","text":""},{"location":"unit1/sparksql/#introduccion","title":"Introducci\u00f3n","text":"<p>Spark SQL es un m\u00f3dulo de Apache Spark que permite ejecutar consultas SQL sobre DataFrames distribuidos. Combina la familiaridad del lenguaje SQL con el rendimiento de Spark y su capacidad de trabajar con grandes vol\u00famenes de datos de forma paralela.</p> <p>En este cap\u00edtulo aprender\u00e1s c\u00f3mo usar SQL en PySpark, c\u00f3mo registrar vistas temporales y c\u00f3mo integrar SQL con transformaciones funcionales.</p>"},{"location":"unit1/sparksql/#que-es-spark-sql","title":"\u00bfQu\u00e9 es Spark SQL?","text":"<p>Spark SQL permite:</p> <ul> <li>Ejecutar consultas SQL est\u00e1ndar sobre DataFrames.</li> <li>Combinar operaciones SQL con transformaciones en Python.</li> <li>Optimizar autom\u00e1ticamente las consultas mediante el Catalyst Optimizer.</li> <li>Integrarse con fuentes como Hive, Parquet, JSON, JDBC, etc.</li> </ul>"},{"location":"unit1/sparksql/#crear-una-vista-temporal","title":"Crear una vista temporal","text":"<p>Para ejecutar SQL sobre un DataFrame, primero debes registrar el DataFrame como una vista:</p> <pre><code>df = spark.read.csv(\"data/ventas.csv\", header=True, inferSchema=True)\n\ndf.createOrReplaceTempView(\"ventas\")\n</code></pre> <p>Esto crea una vista temporal en memoria, similar a una tabla SQL.</p>"},{"location":"unit1/sparksql/#consultar-con-sql","title":"Consultar con SQL","text":"<p>Una vez registrada la vista, puedes usar <code>spark.sql()</code> para realizar consultas:</p> <pre><code>resultado = spark.sql(\"\"\"\n    SELECT cliente, monto\n    FROM ventas\n    WHERE monto &gt; 1000\n    ORDER BY monto DESC\n\"\"\")\n\nresultado.show()\n</code></pre>"},{"location":"unit1/sparksql/#integrar-sql-con-operaciones-funcionales","title":"Integrar SQL con operaciones funcionales","text":"<p>Puedes combinar f\u00e1cilmente consultas SQL con transformaciones de PySpark:</p> <pre><code>ventas_filtradas = spark.sql(\"SELECT * FROM ventas WHERE monto &gt; 1000\")\nventas_filtradas.groupBy(\"cliente\").count().show()\n</code></pre>"},{"location":"unit1/sparksql/#vistas-globales","title":"Vistas globales","text":"<p>Adem\u00e1s de las vistas temporales, puedes usar vistas globales que est\u00e1n disponibles entre sesiones:</p> <pre><code>df.createGlobalTempView(\"ventas_global\")\nspark.sql(\"SELECT * FROM global_temp.ventas_global\").show()\n</code></pre>"},{"location":"unit1/sparksql/#operaciones-sql-comunes","title":"Operaciones SQL comunes","text":"SQL Equivalente PySpark <code>SELECT columna</code> <code>df.select(\"columna\")</code> <code>WHERE</code> <code>df.filter(...)</code> <code>GROUP BY</code> <code>df.groupBy(...).agg(...)</code> <code>ORDER BY</code> <code>df.orderBy(...)</code> <code>JOIN</code> <code>df1.join(df2, on=\"columna\", how=\"inner\")</code>"},{"location":"unit1/sparksql/#cuando-preferir-sql","title":"Cu\u00e1ndo preferir SQL","text":"<p>Spark SQL es \u00fatil cuando:</p> <ul> <li>El equipo tiene experiencia con SQL.</li> <li>Las transformaciones son m\u00e1s legibles en SQL.</li> <li>Se requiere migrar c\u00f3digo SQL existente.</li> </ul> <p>Tambi\u00e9n es com\u00fan en notebooks o dashboards, donde se escriben muchas consultas r\u00e1pidas.</p>"},{"location":"unit1/sparksql/#buenas-practicas","title":"Buenas pr\u00e1cticas","text":"<ul> <li>Usa <code>createOrReplaceTempView()</code> para mantener nombres consistentes.</li> <li>Prefiere SQL cuando las consultas son complejas o se asemejan a l\u00f3gica relacional.</li> <li>Combina SQL con PySpark solo cuando sea necesario para mantener claridad.</li> <li>Usa <code>EXPLAIN</code> en SQL para ver el plan de ejecuci\u00f3n:</li> </ul> <pre><code>spark.sql(\"SELECT * FROM ventas WHERE monto &gt; 1000\").explain()\n</code></pre>"},{"location":"unit1/sparksql/#referencias-utiles","title":"Referencias \u00fatiles","text":"<ul> <li>Spark SQL Documentation</li> <li>Catalyst Optimizer</li> <li>Spark SQL API Reference</li> <li>Using SQL with DataFrames</li> </ul>"},{"location":"unit1/sparksql/#conclusion","title":"Conclusi\u00f3n","text":"<p>Spark SQL es una herramienta poderosa que permite combinar la expresividad de SQL con el poder de procesamiento distribuido de Spark. Es ideal para consultas complejas, manipulaci\u00f3n de grandes datasets y tareas en las que SQL es m\u00e1s natural que la API funcional.</p> <p>En el pr\u00f3ximo cap\u00edtulo veremos c\u00f3mo trabajar con datos en tiempo real usando Spark Streaming.</p>"},{"location":"unit1/streaming/","title":"Spark Streaming","text":""},{"location":"unit1/streaming/#introduccion","title":"Introducci\u00f3n","text":"<p>Adem\u00e1s del procesamiento por lotes (batch), Apache Spark permite procesar flujos de datos en tiempo real a trav\u00e9s de Structured Streaming, un modelo que combina la simplicidad de los DataFrames con la potencia del procesamiento continuo.</p> <p>En este cap\u00edtulo conocer\u00e1s los conceptos b\u00e1sicos de Spark Streaming, c\u00f3mo configurarlo en PySpark, y c\u00f3mo realizar tus primeras operaciones sobre flujos de datos.</p>"},{"location":"unit1/streaming/#que-es-structured-streaming","title":"\u00bfQu\u00e9 es Structured Streaming?","text":"<p>Structured Streaming es un motor de procesamiento de flujos de datos que permite ejecutar consultas incrementales sobre datos en movimiento usando la misma API que los DataFrames.</p> <p>A diferencia de los modelos tradicionales basados en micro-batches puros, Structured Streaming ofrece:</p> <ul> <li>Integraci\u00f3n nativa con la API de DataFrames.</li> <li>Procesamiento tolerante a fallos.</li> <li>Escalabilidad autom\u00e1tica en un cl\u00faster Spark.</li> <li>Soporte para m\u00faltiples fuentes: archivos, Kafka, sockets, etc.</li> </ul>"},{"location":"unit1/streaming/#casos-de-uso-comunes","title":"Casos de uso comunes","text":"<ul> <li>Monitorizaci\u00f3n de logs en tiempo real.</li> <li>Ingesta de datos de sensores (IoT).</li> <li>An\u00e1lisis en tiempo real de clics o eventos de usuarios.</li> <li>Detecci\u00f3n de fraudes o anomal\u00edas.</li> <li>Integraci\u00f3n con Kafka para flujos de alto volumen.</li> </ul>"},{"location":"unit1/streaming/#requisitos-y-limitaciones","title":"Requisitos y limitaciones","text":"<p>Structured Streaming est\u00e1 disponible en Spark 2.0+ y requiere una fuente de datos compatible con streaming, como:</p> <ul> <li>Directorios que reciben archivos nuevos.</li> <li>Sockets de red (para pruebas locales).</li> <li>Conectores como Kafka, Delta Lake o sockets TCP.</li> </ul> <p>No se puede usar <code>show()</code> directamente en un stream. En su lugar, se utilizan \"queries\" activas que escriben en consola, archivos o memoria.</p>"},{"location":"unit1/streaming/#primer-ejemplo-lectura-desde-un-socket","title":"Primer ejemplo: lectura desde un socket","text":"<p>Este ejemplo muestra c\u00f3mo leer texto desde un socket TCP y contar palabras en tiempo real:</p> <pre><code>from pyspark.sql import SparkSession\nfrom pyspark.sql.functions import explode, split\n\nspark = SparkSession.builder.appName(\"StreamingBasico\").getOrCreate()\n\n# Fuente de datos: socket local\nlines = spark.readStream.format(\"socket\") \\\n    .option(\"host\", \"localhost\") \\\n    .option(\"port\", 9999) \\\n    .load()\n\n# Transformaci\u00f3n: contar palabras\nwords = lines.select(explode(split(lines.value, \" \")).alias(\"palabra\"))\nconteo = words.groupBy(\"palabra\").count()\n\n# Sink: escribir en consola\nquery = conteo.writeStream \\\n    .outputMode(\"complete\") \\\n    .format(\"console\") \\\n    .start()\n\nquery.awaitTermination()\n</code></pre> <p>Puedes usar <code>nc -lk 9999</code> en una terminal para enviar texto y probar el ejemplo.</p>"},{"location":"unit1/streaming/#modos-de-salida-outputmode","title":"Modos de salida (<code>outputMode</code>)","text":"<ul> <li><code>append</code>: solo nuevas filas.</li> <li><code>update</code>: solo filas modificadas.</li> <li><code>complete</code>: muestra toda la tabla en cada iteraci\u00f3n.</li> </ul>"},{"location":"unit1/streaming/#consideraciones-importantes","title":"Consideraciones importantes","text":"<ul> <li>El stream se ejecuta de forma continua hasta que se detiene expl\u00edcitamente.</li> <li>Puedes escribir a archivos, memoria, bases de datos o Kafka.</li> <li>Spark maneja autom\u00e1ticamente los micro-batches.</li> </ul>"},{"location":"unit1/streaming/#buenas-practicas","title":"Buenas pr\u00e1cticas","text":"<ul> <li>Siempre especifica un <code>checkpointLocation</code> si el stream es cr\u00edtico (para tolerancia a fallos).</li> <li>Usa <code>outputMode=\"append\"</code> para eficiencia cuando no necesitas toda la tabla.</li> <li>Prueba localmente con sockets antes de pasar a Kafka u otros entornos productivos.</li> <li>No uses <code>collect()</code> ni <code>toPandas()</code> en un stream.</li> </ul>"},{"location":"unit1/streaming/#referencias-utiles","title":"Referencias \u00fatiles","text":"<ul> <li>Structured Streaming Overview</li> <li>API de Structured Streaming</li> <li>Ejemplos de uso</li> </ul>"},{"location":"unit1/streaming/#conclusion","title":"Conclusi\u00f3n","text":"<p>Structured Streaming permite extender el poder de Spark al procesamiento en tiempo real sin cambiar de paradigma. Su integraci\u00f3n con la API de DataFrames facilita el desarrollo de pipelines unificados para datos por lotes y en streaming.</p> <p>En proyectos reales, es com\u00fan combinar Spark SQL y Streaming para construir soluciones completas de an\u00e1lisis en tiempo real.</p>"},{"location":"unit1/transformation/","title":"Transformaciones y Acciones","text":""},{"location":"unit1/transformation/#introduccion","title":"Introducci\u00f3n","text":"<p>Uno de los conceptos m\u00e1s importantes al trabajar con PySpark es la separaci\u00f3n entre transformaciones y acciones. Entender esta diferencia es clave para aprovechar correctamente el modelo de ejecuci\u00f3n diferida (lazy evaluation) y escribir c\u00f3digo eficiente.</p> <p>En este cap\u00edtulo, aprender\u00e1s qu\u00e9 operaciones se consideran transformaciones, cu\u00e1les son acciones, y c\u00f3mo interact\u00faan para construir y ejecutar un trabajo en Spark.</p>"},{"location":"unit1/transformation/#que-son-las-transformaciones","title":"\u00bfQu\u00e9 son las transformaciones?","text":"<p>Las transformaciones definen un nuevo conjunto de datos a partir de uno existente. No se ejecutan inmediatamente, sino que se registran en un plan de ejecuci\u00f3n (DAG) que Spark optimizar\u00e1 y ejecutar\u00e1 m\u00e1s adelante, cuando se invoque una acci\u00f3n.</p> <p>Transformaciones comunes:</p> Transformaci\u00f3n Descripci\u00f3n <code>select()</code> Selecciona columnas <code>filter()</code> Filtra filas seg\u00fan una condici\u00f3n <code>withColumn()</code> A\u00f1ade o transforma una columna <code>drop()</code> Elimina una o m\u00e1s columnas <code>groupBy()</code> Agrupa filas para aplicar agregaciones <code>join()</code> Realiza combinaciones entre DataFrames <code>distinct()</code> Elimina duplicados <code>orderBy()</code> Ordena las filas por una o m\u00e1s columnas <p>Ejemplo de transformaci\u00f3n:</p> <pre><code>df_filtrado = df.filter(df[\"monto\"] &gt; 1000)\n</code></pre>"},{"location":"unit1/transformation/#que-son-las-acciones","title":"\u00bfQu\u00e9 son las acciones?","text":"<p>Las acciones disparan la ejecuci\u00f3n del plan de transformaciones. Cuando se llama a una acci\u00f3n, Spark eval\u00faa todas las transformaciones anteriores, construye un plan f\u00edsico y ejecuta el trabajo en el cl\u00faster.</p> <p>Acciones comunes:</p> Acci\u00f3n Descripci\u00f3n <code>show()</code> Muestra las primeras filas <code>collect()</code> Devuelve todos los resultados al driver (con cuidado) <code>count()</code> Cuenta el n\u00famero de filas <code>first()</code> Retorna la primera fila <code>take(n)</code> Devuelve las primeras <code>n</code> filas <code>write()</code> Escribe los datos en disco <code>foreach()</code> Aplica una funci\u00f3n a cada fila <p>Ejemplo de acci\u00f3n:</p> <pre><code>df_filtrado.show()\n</code></pre>"},{"location":"unit1/transformation/#ejemplo-completo","title":"Ejemplo completo","text":"<pre><code>from pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col\n\nspark = SparkSession.builder.appName(\"TransformacionesAcciones\").getOrCreate()\n\ndf = spark.read.csv(\"data/ventas.csv\", header=True, inferSchema=True)\n\n# Transformaciones encadenadas\ndf_transformado = df.filter(col(\"monto\") &gt; 1000).select(\"cliente\", \"monto\")\n\n# Acci\u00f3n que dispara la ejecuci\u00f3n\ndf_transformado.show()\n</code></pre>"},{"location":"unit1/transformation/#lazy-evaluation-evaluacion-diferida","title":"Lazy Evaluation (Evaluaci\u00f3n Diferida)","text":"<p>Gracias a la separaci\u00f3n entre transformaciones y acciones, Spark puede:</p> <ul> <li>Optimizar el plan de ejecuci\u00f3n.</li> <li>Evitar c\u00e1lculos intermedios innecesarios.</li> <li>Combinar operaciones en etapas m\u00e1s eficientes.</li> </ul> <p>Esto es una gran ventaja frente a otras librer\u00edas que ejecutan cada paso inmediatamente.</p>"},{"location":"unit1/transformation/#buenas-practicas","title":"Buenas pr\u00e1cticas","text":"<ul> <li>Encadena m\u00faltiples transformaciones antes de llamar a una acci\u00f3n.</li> <li>Usa <code>show()</code> en lugar de <code>collect()</code> para evitar traer grandes vol\u00famenes de datos a la memoria local.</li> <li>Evita m\u00faltiples acciones en bucles: cada acci\u00f3n vuelve a ejecutar el plan completo.</li> <li>Usa <code>explain()</code> para ver c\u00f3mo Spark planea ejecutar tus transformaciones.</li> </ul>"},{"location":"unit1/transformation/#referencias-utiles","title":"Referencias \u00fatiles","text":"<ul> <li>Transformations and Actions - Spark Docs</li> <li>Lazy Evaluation Explained</li> <li>API Reference: DataFrame</li> </ul>"},{"location":"unit1/transformation/#conclusion","title":"Conclusi\u00f3n","text":"<p>La separaci\u00f3n entre transformaciones y acciones es central en el modelo de ejecuci\u00f3n de Spark. Esta arquitectura permite diferir la ejecuci\u00f3n, aplicar optimizaciones autom\u00e1ticas y escalar el procesamiento a grandes vol\u00famenes de datos.</p> <p>Dominar esta idea te permitir\u00e1 escribir c\u00f3digo m\u00e1s eficiente y evitar errores comunes al trabajar con PySpark.</p>"},{"location":"unit2/","title":"\u26a1 An\u00e1lisis de Datos con Polars","text":"<p>La Unidad 2 te introduce al procesamiento eficiente de datos tabulares utilizando Polars, una librer\u00eda moderna desarrollada en Rust y dise\u00f1ada para ofrecer alt\u00edsimo rendimiento con una sintaxis declarativa en Python. Aprender\u00e1s a trabajar tanto en modo inmediato (eager) como en modo diferido (lazy), a transformar datos mediante expresiones, y a estructurar pipelines anal\u00edticos limpios y escalables.</p> <p>Esta unidad es clave para comprender c\u00f3mo trabajar con grandes vol\u00famenes de datos sin necesidad de cl\u00fasteres distribuidos, aprovechando al m\u00e1ximo la capacidad de c\u00f3mputo local.</p>"},{"location":"unit2/#objetivos-de-la-unidad","title":"\ud83c\udfaf Objetivos de la unidad","text":"<ul> <li>Comprender la filosof\u00eda y arquitectura de Polars.</li> <li>Cargar y transformar datos usando la API expresiva de Polars.</li> <li>Utilizar expresiones para crear nuevas columnas y condicionar valores.</li> <li>Aplicar agregaciones y agrupaciones avanzadas sobre datos tabulares.</li> <li>Optimizar flujos de trabajo con el modo lazy.</li> <li>Comparar Polars con herramientas tradicionales como Pandas.</li> </ul>"},{"location":"unit2/#contenidos","title":"\ud83d\udcda Contenidos","text":"Tema Descripci\u00f3n breve \u2699\ufe0f Introducci\u00f3n a Polars Qu\u00e9 es Polars, por qu\u00e9 fue creado y en qu\u00e9 contextos se recomienda usar. \ud83d\udcca Operaciones B\u00e1sicas Carga de datos, selecci\u00f3n, filtros y funciones exploratorias. \ud83e\uddf1 Transformaciones con Expresiones Uso de <code>pl.col</code>, <code>pl.when</code>, <code>pl.lit</code>, y c\u00f3mo construir pipelines limpios. \ud83d\udcc8 Agrupaciones y Agregaciones Agrupaciones simples y m\u00faltiples, funciones condicionales y personalizadas. \ud83e\udde0 Lazy Mode Construcci\u00f3n de pipelines diferidos y visualizaci\u00f3n del plan de ejecuci\u00f3n."},{"location":"unit2/#recomendacion","title":"\ud83e\udde0 Recomendaci\u00f3n","text":"<p>Te sugerimos comenzar por la introducci\u00f3n conceptual para familiarizarte con las diferencias entre Polars y Pandas. Luego avanza gradualmente por los cap\u00edtulos t\u00e9cnicos, experimentando en tus propios notebooks con los ejemplos entregados.</p> <p>Para aprovechar completamente el modo lazy, aseg\u00farate de revisar <code>lazy_mode.md</code> despu\u00e9s de entender bien c\u00f3mo funcionan las expresiones en modo eager.</p>"},{"location":"unit2/#prerrequisitos","title":"\ud83d\udcda Prerrequisitos","text":"<ul> <li>Conocimientos b\u00e1sicos de Python y estructuras tipo <code>DataFrame</code></li> <li>Familiaridad con operaciones como filtrado, agrupaci\u00f3n y transformaci\u00f3n de columnas</li> <li>Entorno con Python 3.9+ y Polars instalado (<code>pip install polars</code>)</li> </ul>"},{"location":"unit2/#que-lograras-al-finalizar-esta-unidad","title":"\u2705 \u00bfQu\u00e9 lograr\u00e1s al finalizar esta unidad?","text":"<ul> <li>Manipular datos de forma eficiente con Polars en modo eager y lazy.</li> <li>Escribir pipelines de an\u00e1lisis de datos expresivos y optimizados.</li> <li>Utilizar expresiones declarativas en lugar de c\u00f3digo imperativo.</li> <li>Comparar el rendimiento entre Polars y otras librer\u00edas como Pandas.</li> </ul>"},{"location":"unit2/expresiones/","title":"Transformaciones (expresiones)","text":""},{"location":"unit2/expresiones/#introduccion","title":"Introducci\u00f3n","text":"<p>Polars utiliza un modelo expresivo basado en operaciones sobre columnas, en lugar de manipulaci\u00f3n imperativa de datos. Este enfoque facilita la escritura de pipelines de transformaci\u00f3n m\u00e1s seguros, legibles y r\u00e1pidos.</p> <p>En este cap\u00edtulo aprender\u00e1s a utilizar expresiones (<code>pl.col</code>, <code>pl.when</code>, <code>pl.lit</code>, etc.) para transformar columnas, condicionar valores, agregar m\u00faltiples columnas y componer operaciones complejas de manera eficiente.</p>"},{"location":"unit2/expresiones/#que-es-una-expresion-en-polars","title":"\u00bfQu\u00e9 es una expresi\u00f3n en Polars?","text":"<p>Una expresi\u00f3n es una instrucci\u00f3n que describe c\u00f3mo transformar o computar una columna. Las expresiones son evaluadas dentro de m\u00e9todos como:</p> <ul> <li><code>select()</code></li> <li><code>with_columns()</code></li> <li><code>filter()</code></li> <li><code>groupby().agg()</code></li> </ul> <p>Esto permite separar el qu\u00e9 queremos hacer del cu\u00e1ndo se ejecuta (especialmente \u00fatil en modo lazy).</p>"},{"location":"unit2/expresiones/#seleccionar-columnas-con-plcol","title":"Seleccionar columnas con <code>pl.col</code>","text":"<pre><code>import polars as pl\n\ndf = pl.DataFrame({\n    \"nombre\": [\"Ana\", \"Luis\", \"Pedro\"],\n    \"edad\": [28, 34, 25],\n    \"monto\": [1000, 2500, 1800]\n})\n\ndf.select([\n    pl.col(\"nombre\"),\n    pl.col(\"monto\") * 1.19\n])\n</code></pre>"},{"location":"unit2/expresiones/#crear-nuevas-columnas-con-with_columns","title":"Crear nuevas columnas con <code>with_columns</code>","text":"<pre><code>df = df.with_columns([\n    (pl.col(\"monto\") * 1.19).alias(\"monto_con_iva\"),\n    (pl.col(\"edad\") + 5).alias(\"edad_futura\")\n])\n</code></pre>"},{"location":"unit2/expresiones/#condiciones-con-plwhen","title":"Condiciones con <code>pl.when</code>","text":"<pre><code>df = df.with_columns([\n    pl.when(pl.col(\"monto\") &gt; 2000)\n      .then(\"alta\")\n      .otherwise(\"media\")\n      .alias(\"categoria_compra\")\n])\n</code></pre> <p>Tambi\u00e9n puedes anidar condiciones:</p> <pre><code>df.with_columns([\n    pl.when(pl.col(\"monto\") &gt; 2000).then(\"alta\")\n     .when(pl.col(\"monto\") &gt; 1000).then(\"media\")\n     .otherwise(\"baja\")\n     .alias(\"rango\")\n])\n</code></pre>"},{"location":"unit2/expresiones/#literales-y-funciones-con-pllit","title":"Literales y funciones con <code>pl.lit</code>","text":"<pre><code>df.with_columns([\n    (pl.col(\"edad\") + pl.lit(1)).alias(\"edad_incrementada\")\n])\n</code></pre>"},{"location":"unit2/expresiones/#uso-de-plall-y-plexclude","title":"Uso de <code>pl.all()</code> y <code>pl.exclude()</code>","text":"<pre><code># Aplicar una transformaci\u00f3n a todas las columnas num\u00e9ricas\ndf.select([\n    pl.exclude(\"nombre\") * 2\n])\n</code></pre>"},{"location":"unit2/expresiones/#agregaciones-con-expresiones","title":"Agregaciones con expresiones","text":"<pre><code>df.groupby(\"categoria_compra\").agg([\n    pl.col(\"monto\").mean().alias(\"promedio_monto\"),\n    pl.col(\"monto\").max().alias(\"mayor_compra\")\n])\n</code></pre>"},{"location":"unit2/expresiones/#encadenamiento-de-operaciones","title":"Encadenamiento de operaciones","text":"<pre><code>df = (\n    df.with_columns([\n        (pl.col(\"monto\") * 1.19).alias(\"con_iva\")\n    ])\n    .filter(pl.col(\"edad\") &gt; 30)\n    .select([\"nombre\", \"con_iva\"])\n)\n</code></pre>"},{"location":"unit2/expresiones/#buenas-practicas","title":"Buenas pr\u00e1cticas","text":"<ul> <li>Nombra tus columnas nuevas con <code>.alias()</code> para evitar sobreescribir datos accidentalmente.</li> <li>Encadena expresiones en lugar de escribir m\u00faltiples pasos intermedios.</li> <li>Usa <code>pl.when</code> para l\u00f3gica condicional clara, especialmente en reemplazo de <code>.apply()</code> imperativos.</li> <li>Revisa los tipos de columnas con <code>df.schema</code> para evitar errores de tipo.</li> </ul>"},{"location":"unit2/expresiones/#referencias-utiles","title":"Referencias \u00fatiles","text":"<ul> <li>Expresiones en Polars</li> <li>API de <code>pl.col</code>, <code>pl.when</code>, <code>pl.lit</code></li> <li>Ejemplos del modelo expresivo</li> </ul>"},{"location":"unit2/expresiones/#conclusion","title":"Conclusi\u00f3n","text":"<p>El modelo basado en expresiones es una de las principales ventajas de Polars. Permite construir pipelines de transformaci\u00f3n claros, eficientes y f\u00e1cilmente optimizables. Este enfoque se vuelve a\u00fan m\u00e1s poderoso en el contexto del modo lazy, que veremos en el pr\u00f3ximo cap\u00edtulo.</p>"},{"location":"unit2/groupby/","title":"Agrupaciones y Agregaciones","text":""},{"location":"unit2/groupby/#introduccion","title":"Introducci\u00f3n","text":"<p>Una de las operaciones m\u00e1s comunes en an\u00e1lisis de datos es agrupar por una o m\u00e1s variables y luego aplicar funciones de agregaci\u00f3n, como sumas, promedios o conteos. En Polars, esto se realiza utilizando los m\u00e9todos <code>groupby()</code> y <code>agg()</code> sobre <code>DataFrame</code> o <code>LazyFrame</code>.</p> <p>En este cap\u00edtulo profundizaremos en c\u00f3mo usar estas funciones de forma eficiente y expresiva, incluyendo m\u00faltiples agregaciones, alias, condiciones y expresiones personalizadas.</p>"},{"location":"unit2/groupby/#estructura-basica","title":"Estructura b\u00e1sica","text":"<pre><code>df.groupby(\"columna\").agg([\n    pl.col(\"otra_columna\").sum()\n])\n</code></pre> <p>Esto agrupa por <code>\"columna\"</code> y suma los valores de <code>\"otra_columna\"</code>.</p>"},{"location":"unit2/groupby/#ejemplo-practico","title":"Ejemplo pr\u00e1ctico","text":"<pre><code>import polars as pl\n\ndf = pl.DataFrame({\n    \"cliente\": [\"Ana\", \"Luis\", \"Ana\", \"Luis\", \"Pedro\"],\n    \"monto\": [100, 250, 300, 400, 150],\n    \"producto\": [\"A\", \"B\", \"A\", \"C\", \"B\"]\n})\n\ndf.groupby(\"cliente\").agg([\n    pl.col(\"monto\").sum().alias(\"total\"),\n    pl.count().alias(\"compras\")\n])\n</code></pre>"},{"location":"unit2/groupby/#agrupaciones-multiples","title":"Agrupaciones m\u00faltiples","text":"<p>Puedes agrupar por m\u00e1s de una columna:</p> <pre><code>df.groupby([\"cliente\", \"producto\"]).agg([\n    pl.col(\"monto\").mean().alias(\"promedio_monto\")\n])\n</code></pre>"},{"location":"unit2/groupby/#varias-agregaciones-sobre-la-misma-columna","title":"Varias agregaciones sobre la misma columna","text":"<pre><code>df.groupby(\"cliente\").agg([\n    pl.col(\"monto\").sum(),\n    pl.col(\"monto\").mean(),\n    pl.col(\"monto\").max()\n])\n</code></pre>"},{"location":"unit2/groupby/#agregaciones-condicionales","title":"Agregaciones condicionales","text":"<p>Puedes usar expresiones condicionales dentro de una agregaci\u00f3n:</p> <pre><code>df.groupby(\"cliente\").agg([\n    (pl.when(pl.col(\"monto\") &gt; 200)\n       .then(1)\n       .otherwise(0)\n    ).sum().alias(\"compras_altas\")\n])\n</code></pre> <p>Esto cuenta cu\u00e1ntas compras superaron los 200 por cliente.</p>"},{"location":"unit2/groupby/#agrupacion-sobre-multiples-columnas-dinamicamente","title":"Agrupaci\u00f3n sobre m\u00faltiples columnas din\u00e1micamente","text":"<p>Con <code>pl.all()</code> puedes aplicar una operaci\u00f3n a todas las columnas (excepto las agrupadas):</p> <pre><code>df.groupby(\"producto\").agg([\n    pl.all().sum()\n])\n</code></pre> <p>Tambi\u00e9n puedes excluir columnas:</p> <pre><code>df.groupby(\"producto\").agg([\n    pl.exclude(\"cliente\").mean()\n])\n</code></pre>"},{"location":"unit2/groupby/#agregaciones-con-funciones-personalizadas","title":"Agregaciones con funciones personalizadas","text":"<p>Puedes aplicar funciones definidas por ti:</p> <pre><code>def rango(col):\n    return col.max() - col.min()\n\ndf.groupby(\"cliente\").agg([\n    rango(pl.col(\"monto\")).alias(\"rango_montos\")\n])\n</code></pre>"},{"location":"unit2/groupby/#uso-en-lazyframe","title":"Uso en LazyFrame","text":"<p>La sintaxis es la misma, pero recuerda finalizar con <code>.collect()</code>:</p> <pre><code>df.lazy().groupby(\"cliente\").agg([\n    pl.col(\"monto\").sum()\n]).collect()\n</code></pre>"},{"location":"unit2/groupby/#buenas-practicas","title":"Buenas pr\u00e1cticas","text":"<ul> <li>Usa <code>.alias()</code> para nombrar las columnas agregadas de forma clara.</li> <li>Evita calcular varias veces la misma expresi\u00f3n (reutiliza con <code>alias()</code>).</li> <li>Prefiere agrupar por m\u00faltiples columnas si existen relaciones jer\u00e1rquicas (cliente + producto).</li> <li>En modo <code>lazy</code>, agrupar primero puede mejorar la eficiencia si reduces el volumen de datos temprano.</li> </ul>"},{"location":"unit2/groupby/#referencias-utiles","title":"Referencias \u00fatiles","text":"<ul> <li>Polars API: groupby</li> <li>Agregaciones disponibles</li> <li>Gu\u00eda oficial de agregaciones</li> </ul>"},{"location":"unit2/groupby/#conclusion","title":"Conclusi\u00f3n","text":"<p>Las operaciones de agrupaci\u00f3n y agregaci\u00f3n en Polars son extremadamente expresivas y eficientes, tanto en modo eager como lazy. Su integraci\u00f3n con expresiones permite construir an\u00e1lisis complejos con un c\u00f3digo claro y conciso.</p> <p>Dominar <code>groupby()</code> + <code>agg()</code> es fundamental para todo trabajo de an\u00e1lisis exploratorio, reporting y preparaci\u00f3n de datos para machine learning.</p>"},{"location":"unit2/introduction/","title":"Introducci\u00f3n a Polars","text":""},{"location":"unit2/introduction/#introduccion","title":"Introducci\u00f3n","text":"<p>Polars es una librer\u00eda moderna para el an\u00e1lisis de datos en Python y Rust. Est\u00e1 dise\u00f1ada para ser extremadamente r\u00e1pida, eficiente en memoria y escalable, ofreciendo una alternativa moderna a Pandas, especialmente \u00fatil cuando se trabaja con datasets grandes o se necesita alto rendimiento.</p> <p>En este cap\u00edtulo exploraremos qu\u00e9 es Polars, por qu\u00e9 ha ganado popularidad en la comunidad de ciencia de datos y c\u00f3mo se compara con otras librer\u00edas como Pandas y PySpark.</p>"},{"location":"unit2/introduction/#que-es-polars","title":"\u00bfQu\u00e9 es Polars?","text":"<p>Polars es una librer\u00eda de procesamiento de datos columnar construida sobre Rust, con bindings en Python. Soporta un enfoque expresivo basado en expresiones (similar a SQL) y ofrece dos modos de ejecuci\u00f3n:</p> <ul> <li>Eager (modo inmediato): estilo similar a Pandas, \u00fatil para exploraci\u00f3n interactiva.</li> <li>Lazy (modo diferido): permite construir pipelines de transformaci\u00f3n optimizados autom\u00e1ticamente antes de ejecutarse.</li> </ul> <p>Polars es particularmente \u00fatil cuando se requiere:</p> <ul> <li>Rendimiento extremo sin necesidad de un cl\u00faster.</li> <li>Bajo uso de memoria en comparaci\u00f3n con Pandas.</li> <li>Escritura concisa y expresiva de transformaciones complejas.</li> </ul>"},{"location":"unit2/introduction/#por-que-usar-polars","title":"\u00bfPor qu\u00e9 usar Polars?","text":"<p>Polars resuelve muchas de las limitaciones de Pandas:</p> Limitaci\u00f3n de Pandas Soluci\u00f3n en Polars Alto uso de memoria Uso eficiente de estructuras columnar en Rust Poca velocidad en grandes datasets Motor optimizado, multithreading por defecto C\u00f3digo imperativo y repetitivo Transformaciones declarativas con expresiones Sin ejecuci\u00f3n diferida Lazy mode para optimizaci\u00f3n autom\u00e1tica"},{"location":"unit2/introduction/#comparacion-inicial-con-pandas","title":"Comparaci\u00f3n inicial con Pandas","text":"<pre><code># En Pandas\ndf[df[\"columna\"] &gt; 100][\"otra\"] = df[\"otra\"] * 2\n\n# En Polars (modo expresivo)\ndf = df.with_columns(\n    (pl.col(\"otra\") * 2).alias(\"otra\")\n).filter(pl.col(\"columna\") &gt; 100)\n</code></pre> <p>Aunque m\u00e1s expl\u00edcito, el modelo de expresiones de Polars evita errores comunes, es m\u00e1s legible en flujos complejos y m\u00e1s eficiente.</p>"},{"location":"unit2/introduction/#instalacion","title":"Instalaci\u00f3n","text":"<p>Puedes instalar Polars f\u00e1cilmente con pip:</p> <pre><code>pip install polars\n</code></pre> <p>Para usar la versi\u00f3n con soporte de Parquet, Arrow y Lazy:</p> <pre><code>pip install polars[lazy]\n</code></pre>"},{"location":"unit2/introduction/#casos-de-uso-ideales-para-polars","title":"Casos de uso ideales para Polars","text":"<ul> <li>An\u00e1lisis de datos tabulares de tama\u00f1o medio a grande.</li> <li>Reemplazo de pipelines lentos en Pandas.</li> <li>Preprocesamiento de datos en entornos de despliegue o backend.</li> <li>Ejecuci\u00f3n local de transformaciones sin necesidad de Spark o cl\u00fasteres distribuidos.</li> </ul>"},{"location":"unit2/introduction/#limitaciones-actuales","title":"Limitaciones actuales","text":"<ul> <li>Comunidad m\u00e1s peque\u00f1a que Pandas (aunque en crecimiento).</li> <li>Algunas funciones estad\u00edsticas avanzadas no est\u00e1n disponibles.</li> <li>La API puede resultar menos familiar al principio por su estilo funcional y expresivo.</li> </ul>"},{"location":"unit2/introduction/#referencias-utiles","title":"Referencias \u00fatiles","text":"<ul> <li>Sitio oficial de Polars</li> <li>Documentaci\u00f3n en Python</li> <li>Comparaci\u00f3n Polars vs Pandas</li> <li>Polars GitHub</li> </ul>"},{"location":"unit2/introduction/#conclusion","title":"Conclusi\u00f3n","text":"<p>Polars es una excelente alternativa a Pandas cuando necesitas m\u00e1s velocidad, menor uso de memoria o una sintaxis m\u00e1s declarativa y robusta para tus transformaciones. Su dise\u00f1o en Rust le permite competir con librer\u00edas como PySpark en escenarios locales, pero con mucha mayor simplicidad.</p> <p>En los pr\u00f3ximos cap\u00edtulos exploraremos c\u00f3mo trabajar con Polars en la pr\u00e1ctica: cargando datos, aplicando transformaciones y usando expresiones poderosas para procesar informaci\u00f3n de forma eficiente.</p>"},{"location":"unit2/lazy_mode/","title":"Lazy Mode","text":""},{"location":"unit2/lazy_mode/#introduccion","title":"Introducci\u00f3n","text":"<p>Polars ofrece dos modos de ejecuci\u00f3n: eager (inmediato) y lazy (evaluaci\u00f3n diferida). Mientras que en el modo eager cada operaci\u00f3n se ejecuta inmediatamente, en el modo lazy las operaciones se registran y se ejecutan solo cuando es necesario, permitiendo aplicar optimizaciones autom\u00e1ticas.</p> <p>Este enfoque es similar al que usan motores como Apache Spark y permite obtener mejores tiempos de ejecuci\u00f3n y menor consumo de memoria, especialmente en pipelines complejos.</p>"},{"location":"unit2/lazy_mode/#que-es-el-modo-lazy","title":"\u00bfQu\u00e9 es el modo lazy?","text":"<p>El modo lazy permite construir un plan de ejecuci\u00f3n completo que luego es optimizado por Polars antes de ejecutarse. Esto incluye:</p> <ul> <li>Eliminaci\u00f3n de pasos innecesarios.</li> <li>Reordenamiento de operaciones para mejorar eficiencia.</li> <li>Ejecuci\u00f3n en paralelo en m\u00faltiples n\u00facleos.</li> </ul> <p>Es ideal cuando necesitas encadenar m\u00faltiples transformaciones, leer archivos grandes o automatizar procesos de preprocesamiento.</p>"},{"location":"unit2/lazy_mode/#crear-un-lazyframe","title":"Crear un LazyFrame","text":"<pre><code>import polars as pl\n\nlf = pl.read_csv(\"data/ventas.csv\").lazy()\n</code></pre> <p>Esto no ejecuta la lectura del archivo todav\u00eda. Solo crea un objeto <code>LazyFrame</code> que contiene el plan.</p>"},{"location":"unit2/lazy_mode/#encadenar-transformaciones","title":"Encadenar transformaciones","text":"<pre><code>lf = (\n    pl.read_csv(\"data/ventas.csv\").lazy()\n    .filter(pl.col(\"monto\") &gt; 1000)\n    .with_columns([\n        (pl.col(\"monto\") * 1.19).alias(\"monto_con_iva\")\n    ])\n    .select([\"cliente\", \"monto_con_iva\"])\n)\n</code></pre> <p>A\u00fan no se ha le\u00eddo ni procesado el archivo. El plan se ejecuta cuando se llama a <code>.collect()</code>:</p> <pre><code>df = lf.collect()\n</code></pre>"},{"location":"unit2/lazy_mode/#visualizar-el-plan-de-ejecucion","title":"Visualizar el plan de ejecuci\u00f3n","text":"<pre><code>lf.explain()\n</code></pre> <p>Esto muestra el plan l\u00f3gico que Polars construy\u00f3. Es muy \u00fatil para entender c\u00f3mo se optimiza el flujo.</p>"},{"location":"unit2/lazy_mode/#optimizacion-automatica","title":"Optimizaci\u00f3n autom\u00e1tica","text":"<p>El motor de Polars aplicar\u00e1 optimizaciones como:</p> <ul> <li>Proyecci\u00f3n empujada: eliminar columnas innecesarias.</li> <li>Filtrado empujado: aplicar filtros lo antes posible.</li> <li>Combinaci\u00f3n de pasos: agrupar operaciones para evitar materializaciones intermedias.</li> </ul> <p>Estas optimizaciones son completamente autom\u00e1ticas y no requieren intervenci\u00f3n del usuario.</p>"},{"location":"unit2/lazy_mode/#comparacion-eager-vs-lazy","title":"Comparaci\u00f3n Eager vs Lazy","text":"<pre><code># Eager\ndf = pl.read_csv(\"data/ventas.csv\")\ndf = df.filter(pl.col(\"monto\") &gt; 1000)\ndf = df.select([\"cliente\", \"monto\"])\n\n# Lazy\nlf = (\n    pl.read_csv(\"data/ventas.csv\").lazy()\n    .filter(pl.col(\"monto\") &gt; 1000)\n    .select([\"cliente\", \"monto\"])\n)\ndf = lf.collect()\n</code></pre> <p>En modo eager, cada paso se ejecuta de inmediato. En modo lazy, se construye el pipeline completo antes de ejecutar.</p>"},{"location":"unit2/lazy_mode/#recomendaciones-para-usar-lazy","title":"Recomendaciones para usar Lazy","text":"<ul> <li>Lectura y transformaci\u00f3n de archivos grandes (CSV, Parquet, etc.).</li> <li>Preprocesamiento en pipelines de ML.</li> <li>Proyectos en producci\u00f3n donde la eficiencia es clave.</li> <li>Validaci\u00f3n o limpieza de datos en pasos encadenados.</li> </ul>"},{"location":"unit2/lazy_mode/#buenas-practicas","title":"Buenas pr\u00e1cticas","text":"<ul> <li>Siempre finaliza con <code>.collect()</code> para materializar el resultado.</li> <li>Usa <code>.explain()</code> si tienes dudas sobre la eficiencia de tu pipeline.</li> <li>Mant\u00e9n la l\u00f3gica declarativa y evita operaciones fuera del pipeline.</li> <li>Combina <code>with_columns</code>, <code>select</code>, <code>filter</code>, y <code>groupby</code> para expresar todo dentro del flujo lazy.</li> </ul>"},{"location":"unit2/lazy_mode/#referencias-utiles","title":"Referencias \u00fatiles","text":"<ul> <li>Documentaci\u00f3n oficial del modo Lazy</li> <li>Explicaci\u00f3n de optimizaciones autom\u00e1ticas</li> <li>Ejemplos avanzados de LazyFrame</li> </ul>"},{"location":"unit2/lazy_mode/#conclusion","title":"Conclusi\u00f3n","text":"<p>El modo lazy de Polars permite construir pipelines declarativos y eficientes, optimizados autom\u00e1ticamente antes de ejecutarse. Esta caracter\u00edstica es especialmente valiosa al trabajar con grandes vol\u00famenes de datos o al desarrollar flujos de procesamiento reutilizables y escalables.</p> <p>Dominar el modo lazy es clave para aprovechar todo el potencial de Polars como herramienta de ingenier\u00eda de datos de alto rendimiento.</p>"},{"location":"unit2/operaciones_basicas/","title":"Operaciones b\u00e1sicas","text":""},{"location":"unit2/operaciones_basicas/#introduccion","title":"Introducci\u00f3n","text":"<p>Polars proporciona una interfaz r\u00e1pida y expresiva para trabajar con datos tabulares. En este cap\u00edtulo aprender\u00e1s a cargar datos, explorar columnas, realizar transformaciones comunes y exportar resultados utilizando la API en modo eager (evaluaci\u00f3n inmediata).</p>"},{"location":"unit2/operaciones_basicas/#importar-polars","title":"Importar Polars","text":"<p>Antes de comenzar, aseg\u00farate de tener instalada la librer\u00eda:</p> <pre><code>pip install polars\n</code></pre> <p>Y luego, en tu script o notebook:</p> <pre><code>import polars as pl\n</code></pre>"},{"location":"unit2/operaciones_basicas/#crear-un-dataframe-desde-una-lista","title":"Crear un DataFrame desde una lista","text":"<pre><code>df = pl.DataFrame({\n    \"nombre\": [\"Ana\", \"Luis\", \"Pedro\"],\n    \"edad\": [28, 34, 25],\n    \"ciudad\": [\"Valpara\u00edso\", \"Santiago\", \"La Serena\"]\n})\n\nprint(df)\n</code></pre>"},{"location":"unit2/operaciones_basicas/#leer-un-archivo-csv","title":"Leer un archivo CSV","text":"<pre><code>df = pl.read_csv(\"data/ventas.csv\")\ndf.head(5)\n</code></pre> <p>Par\u00e1metros comunes:</p> <ul> <li><code>has_header=True</code>: usa la primera fila como encabezado (por defecto).</li> <li><code>infer_schema_length=1000</code>: filas a usar para inferir tipos.</li> <li><code>separator=\";\"</code>: separador personalizado.</li> </ul>"},{"location":"unit2/operaciones_basicas/#inspeccionar-el-dataframe","title":"Inspeccionar el DataFrame","text":"<pre><code>df.shape          # (n_filas, n_columnas)\ndf.columns        # Lista de nombres de columnas\ndf.schema         # Diccionario con nombre y tipo de cada columna\ndf.describe()     # Estad\u00edsticas b\u00e1sicas\n</code></pre>"},{"location":"unit2/operaciones_basicas/#seleccionar-y-renombrar-columnas","title":"Seleccionar y renombrar columnas","text":"<pre><code>df.select([\"cliente\", \"monto\"])  # Selecci\u00f3n simple\n\ndf = df.rename({\"monto\": \"total_compra\"})  # Renombrar columnas\n</code></pre>"},{"location":"unit2/operaciones_basicas/#filtrar-filas","title":"Filtrar filas","text":"<pre><code>df.filter(pl.col(\"monto\") &gt; 1000)\n\n# Combinar condiciones\ndf.filter(\n    (pl.col(\"monto\") &gt; 1000) &amp; (pl.col(\"ciudad\") == \"Santiago\")\n)\n</code></pre>"},{"location":"unit2/operaciones_basicas/#crear-nuevas-columnas","title":"Crear nuevas columnas","text":"<pre><code>df = df.with_columns([\n    (pl.col(\"monto\") * 1.19).alias(\"monto_con_iva\")\n])\n</code></pre>"},{"location":"unit2/operaciones_basicas/#agrupar-y-agregar","title":"Agrupar y agregar","text":"<pre><code>df.groupby(\"cliente\").agg([\n    pl.col(\"monto\").sum().alias(\"total\"),\n    pl.col(\"monto\").mean().alias(\"promedio\")\n])\n</code></pre>"},{"location":"unit2/operaciones_basicas/#ordenar-resultados","title":"Ordenar resultados","text":"<pre><code>df.sort(\"monto\", descending=True)\n</code></pre>"},{"location":"unit2/operaciones_basicas/#eliminar-columnas","title":"Eliminar columnas","text":"<pre><code>df.drop(\"columna_innecesaria\")\n</code></pre>"},{"location":"unit2/operaciones_basicas/#exportar-a-csv","title":"Exportar a CSV","text":"<pre><code>df.write_csv(\"data/output.csv\")\n</code></pre> <p>Tambi\u00e9n puedes exportar a otros formatos como Parquet o JSON:</p> <pre><code>df.write_parquet(\"data/output.parquet\")\ndf.write_json(\"data/output.json\")\n</code></pre>"},{"location":"unit2/operaciones_basicas/#buenas-practicas","title":"Buenas pr\u00e1cticas","text":"<ul> <li>Usa <code>pl.Config.set_tbl_rows(n)</code> para controlar cu\u00e1ntas filas muestra por defecto.</li> <li>Evita <code>print(df)</code> para datasets grandes: usa <code>df.head()</code> o <code>df.sample(n)</code>.</li> <li>Encadena transformaciones usando <code>with_columns</code>, <code>filter</code>, <code>groupby</code>, etc.</li> <li>Considera el modo lazy para optimizar pipelines m\u00e1s complejos (lo veremos en el pr\u00f3ximo cap\u00edtulo).</li> </ul>"},{"location":"unit2/operaciones_basicas/#referencias-utiles","title":"Referencias \u00fatiles","text":"<ul> <li>API de Polars: DataFrame</li> <li>Gu\u00eda de lectura y escritura</li> <li>Ejemplos pr\u00e1cticos de uso</li> </ul>"},{"location":"unit2/operaciones_basicas/#conclusion","title":"Conclusi\u00f3n","text":"<p>El modo eager de Polars permite ejecutar operaciones comunes de an\u00e1lisis de datos de manera r\u00e1pida y eficiente. Su sintaxis, basada en expresiones y columnas, ofrece claridad y control, ideal para an\u00e1lisis exploratorios y procesamiento de datos estructurados.</p> <p>En el pr\u00f3ximo cap\u00edtulo veremos c\u00f3mo usar el modo lazy, que permite optimizar y encadenar m\u00faltiples operaciones sin ejecutarlas inmediatamente.</p>"}]}