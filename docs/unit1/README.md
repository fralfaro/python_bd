# 🔥 Fundamentos de PySpark

La Unidad 1 te introduce al mundo del procesamiento distribuido con **Apache Spark** usando Python. Aquí aprenderás a manejar grandes volúmenes de datos de manera eficiente, utilizando transformaciones, acciones y consultas SQL sobre estructuras distribuidas. Esta unidad es clave para trabajar con Big Data de forma escalable y profesional.

---

## 🎯 Objetivos de la unidad

- Comprender el modelo de ejecución distribuido de Spark.
- Crear y transformar estructuras de datos distribuidas (RDD y DataFrame).
- Ejecutar operaciones con `transformations` y `actions`.
- Consultar datos usando Spark SQL.
- Comprender el flujo de trabajo de un job de Spark.
- Integrar Spark en entornos locales o en la nube.

---

## 📚 Contenidos

| Tema | Descripción breve |
|------|--------------------|
| ⚙️ **Introducción a Spark y PySpark** | Qué es Spark, cómo funciona y cómo lo usamos con Python. |
| 📦 **RDD y DataFrames** | Tipos de estructuras distribuidas, diferencias y casos de uso. |
| 🔁 **Transformaciones y Acciones** | Cómo manipular y procesar los datos de forma eficiente. |
| 🧠 **Lazy Evaluation** | Ejecución diferida y optimización automática del plan de ejecución. |
| 🔍 **Spark SQL** | Consultas tipo SQL sobre grandes conjuntos de datos. |
| 🧪 **Spark Streaming (básico)** | Introducción al procesamiento de datos en tiempo real. |

---

## 🧠 Recomendación

Te sugerimos comenzar por la introducción conceptual y luego avanzar paso a paso por las transformaciones, SQL y finalmente streaming. Asegúrate de **ejecutar los ejemplos tú mismo** para entender cómo Spark distribuye las operaciones.

---

## 📚 Prerrequisitos

- Conocimientos intermedios de Python
- Familiaridad con estructuras tipo `DataFrame`
- Entorno configurado para ejecutar PySpark

---

## ✅ ¿Qué lograrás al finalizar esta unidad?

- Procesar grandes volúmenes de datos con PySpark.
- Utilizar transformaciones y acciones de forma eficiente.
- Consultar datos con Spark SQL.
- Comprender cómo funciona un motor de ejecución distribuido.
