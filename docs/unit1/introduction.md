#  Introducci칩n a PySpark

<img src="../images/pyspark.png" alt="Banner del Curso" width="500" >


## Introducci칩n

Apache Spark es uno de los motores de procesamiento m치s potentes y utilizados en el ecosistema Big Data. Su capacidad para procesar grandes vol칰menes de datos de manera distribuida, r치pida y en memoria lo convierte en una herramienta clave en la ingenier칤a de datos moderna.

En este cap칤tulo, exploraremos qu칠 es PySpark, por qu칠 es relevante en proyectos de datos a gran escala y c칩mo se posiciona frente a otras herramientas tradicionales como Pandas.

---

## 쯈u칠 es Apache Spark?

Apache Spark es un motor de an치lisis de datos distribuido de c칩digo abierto, dise침ado para procesar grandes vol칰menes de datos de manera paralela y eficiente.

Fue creado en la Universidad de California, Berkeley, como una evoluci칩n del modelo MapReduce de Hadoop. A diferencia de este 칰ltimo, Spark permite procesamiento en memoria (in-memory), lo que reduce significativamente los tiempos de ejecuci칩n.

Spark permite trabajar con distintos tipos de procesamiento:

- **Batch** (por lotes)
- **Streaming** (en tiempo real)
- **Machine Learning**
- **SQL**
- **Gr치ficos** (GraphX)

---

## 쯈u칠 es PySpark?

**PySpark** es la interfaz de Python para Apache Spark. Gracias a PySpark, los desarrolladores Python pueden escribir c칩digo para trabajar con Spark sin necesidad de utilizar Scala o Java.

Entre sus capacidades m치s destacadas est치n:

- Trabajar con DataFrames distribuidos (similar a Pandas, pero escalable).
- Ejecutar operaciones SQL con `Spark SQL`.
- Procesar datos en tiempo real con `Structured Streaming`.
- Realizar tareas de machine learning con `MLlib`.

> 游댢 PySpark traduce las instrucciones escritas en Python al n칰cleo de ejecuci칩n de Spark, lo que permite trabajar a gran escala sin perder la flexibilidad de Python.

---

## 쯇or qu칠 usar PySpark en lugar de Pandas?

Aunque **Pandas** es excelente para an치lisis de datos en memoria, presenta limitaciones cuando:

- Los datos superan los **8-16GB** (el tama침o de la RAM).
- Se requiere procesamiento paralelo o distribuido.
- Se necesita conectar m칰ltiples fuentes de datos grandes.

| Caracter칤stica     | Pandas               | PySpark                      |
|--------------------|----------------------|------------------------------|
| Tama침o de datos    | En memoria (limitado)| Escalable a cl칰ster         |
| Paralelismo        | No                   | S칤, distribuido              |
| Tipado             | Din치mico             | Est치tico (inferido)          |
| Tiempo de carga    | R치pido en peque침os   | Optimizado para grandes      |
| Lenguaje base      | Python puro          | Python sobre Spark (Scala)   |

---

## Casos de uso t칤picos para PySpark

- Procesamiento de logs de servidores o aplicaciones.
- Limpieza y transformaci칩n de datasets de terabytes.
- Preparaci칩n de datos para modelos de machine learning distribuidos.
- An치lisis de comportamiento de usuarios en tiempo real.
- Consultas complejas sobre archivos Parquet, ORC, CSV distribuidos.

---

## Primer vistazo al c칩digo PySpark

Este es un ejemplo b치sico para iniciar una sesi칩n de Spark y leer un archivo:

```python
from pyspark.sql import SparkSession

# Crear una SparkSession
spark = SparkSession.builder.appName("MiPrimerPySpark").getOrCreate()

# Leer un CSV (puede ser local o desde Google Drive)
df = spark.read.csv("data/ventas.csv", header=True, inferSchema=True)

# Mostrar las primeras filas
df.show()
```

> 游빍 Puedes probar este ejemplo f치cilmente en Google Colab con `!pip install pyspark`.

---

## Ventajas principales de PySpark

- **Escalabilidad horizontal:** puedes escalar tu trabajo desde tu laptop hasta cientos de nodos en un cl칰ster.
- **Lenguaje conocido:** no necesitas aprender Java o Scala.
- **Ecosistema rico:** acceso a Spark SQL, MLlib, GraphFrames, entre otros.
- **Interoperabilidad:** puede conectarse con Hadoop, Hive, Kafka, Cassandra, Delta Lake, etc.


---

## Referencias 칰tiles

- [Apache Spark - Sitio oficial](https://spark.apache.org/)
- [Documentaci칩n de PySpark](https://spark.apache.org/docs/latest/api/python/)
- [Comparativa PySpark vs Pandas (Databricks)](https://www.databricks.com/glossary/pyspark-vs-pandas)
- [Gu칤a de instalaci칩n en Colab (pyspark)](https://colab.research.google.com/github/databricks/koalas/blob/master/docs/source/user_guide/10min.ipynb)

---

## Conclusi칩n

PySpark es una herramienta fundamental en la ingenier칤a de datos moderna. Su combinaci칩n de potencia, escalabilidad y compatibilidad con Python lo convierten en una opci칩n natural para procesar y analizar grandes vol칰menes de datos.

Dominar PySpark abre la puerta a trabajar con proyectos de Big Data reales, donde Pandas ya no es suficiente. En los pr칩ximos cap칤tulos, aprender치s a construir DataFrames, ejecutar transformaciones distribuidas y optimizar tus consultas como un profesional.
